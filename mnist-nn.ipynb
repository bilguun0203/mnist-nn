{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Neural Network - MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Өгөгдөл унших"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mnist\n",
    "import helper\n",
    "import math\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "training_images, training_labels, test_images, test_labels = mnist.load()\n",
    "\n",
    "def nanCheck(x, msg):\n",
    "#     print(type(x))\n",
    "    if type(x) is list or type(x) is np.ndarray:\n",
    "#         print('--1if')\n",
    "#         print(type(x[0]))\n",
    "        if type(x[0]) is list:\n",
    "#             print('----2if')\n",
    "            for i, t in enumerate(x):\n",
    "#                 print('------for')\n",
    "                if np.isnan(np.array(t, dtype=np.float64)).any():\n",
    "                    print(x)\n",
    "                    raise ValueError('NaN ' + str(i) + ': ' + msg)\n",
    "            return None\n",
    "        if type(x[0]) is np.ndarray:\n",
    "#             print('----3if')\n",
    "            for i, t in enumerate(x):\n",
    "#                 print('------for')\n",
    "                if np.isnan(t).any():\n",
    "                    print(x)\n",
    "                    raise ValueError('NaN ' + str(i) + ': ' + msg)\n",
    "            return None\n",
    "#     print('--out')\n",
    "#     if type(x) is np.ndarray:\n",
    "#         if np.isnan(x).any():\n",
    "#             raise ValueError('NaN ' + str(i) + ': ' + msg)\n",
    "#             return None\n",
    "#     else:\n",
    "    try:\n",
    "        if np.isnan(np.array(x, dtype=np.float64)).any():\n",
    "            print(x)\n",
    "            raise ValueError('NaN: ' + msg)\n",
    "    except:\n",
    "        print(x)\n",
    "        raise ValueError('NaN: ' + msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тооцоолол хийхэд хэрэг болох функцууд"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RELU\n",
    "\n",
    "\\begin{equation*}\n",
    "relu(y_i) = \\begin{cases} 0, & y_i < 0 \\\\ y_i, & y_i\\geq 0 \\end{cases}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(y):\n",
    "    nanCheck(y, 'relu - y')\n",
    "    y[y <= 0] = 0\n",
    "    nanCheck(y, 'relu - relu(y)')\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "\\begin{equation*}\n",
    "softmax(y_i) = \\frac{e^{y_i}}{\\sum_{j}e_j}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y):\n",
    "#     print(y)\n",
    "    nanCheck(y, 'softmax - y')\n",
    "    e = [np.exp(i) for i in y]\n",
    "    nanCheck(e, 'softmax - e')\n",
    "    sum_of_e = sum(e)\n",
    "    nanCheck(sum_of_e, 'softmax - sum_of_e')\n",
    "    res = [i / sum_of_e for i in e]\n",
    "    nanCheck(res, 'softmax - res')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нейронуудын утгыг тооцох\n",
    "\n",
    "- W - тухайн давхаргын weight\n",
    "- X - өмнөх нейронуудын утга\n",
    "- b - тухайн давхаргын bias\n",
    "\\begin{equation*}\n",
    "W X + b\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_neuron(x, weight, bias):\n",
    "    nanCheck(x, 'calculate_neuron - x')\n",
    "    nanCheck(weight, 'calculate_neuron - weight')\n",
    "    nanCheck(bias, 'calculate_neuron - bias')\n",
    "    return x @ weight + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Log Likelihood\n",
    "Loss бодоход ашиглана.\n",
    "- Y* - жинхэнэ ангилал\n",
    "- Y - бодсон ангилал\n",
    "- n - сургалтын өгөгдлийн тоо\n",
    "\\begin{equation*}\n",
    "- \\sum_{n} \\sum_{i} Y_{ni}^* log(Y_{ni})\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(y, Y):\n",
    "    sum = 0\n",
    "    for (yi, Yi) in zip(y, Y):\n",
    "#         print(yi, Yi)\n",
    "        l = np.multiply(np.log(yi), Yi)\n",
    "#         print(np.log(yi))\n",
    "        nanCheck(l, 'NLL - Log')\n",
    "        sum = sum + np.sum(l)\n",
    "        nanCheck(l, 'NLL - Sum')\n",
    "    return -(sum/len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(weights, biases, neurons, neurons_activated, prediction, target):\n",
    "    dW = []\n",
    "    db = []\n",
    "    d = [np.subtract(prediction, target)]\n",
    "#     print('------Z------')\n",
    "#     print(prediction)\n",
    "#     print(target)\n",
    "#     print(dz)\n",
    "#     print('------ZZZ-----')\n",
    "    m = len(d)\n",
    "    nanCheck(d, 'sgd - dz[0]')\n",
    "    dW.append(np.multiply(1/m, (np.transpose(neurons[1]) @ d[0])))\n",
    "    nanCheck(dW, 'sgd - dW[0]')\n",
    "    db.append(np.multiply(1/m, np.sum(d[0], axis=0)))\n",
    "    nanCheck(db, 'sgd - db[0]')\n",
    "    d.append(np.multiply(d[0] @ np.transpose(weights[1]), np.heaviside(neurons_activated[1], 1)))\n",
    "#     print('22222222222222222222222222222222')\n",
    "#     print(dz[len(dz)-1].shape)\n",
    "#     print(neurons[0])\n",
    "#     print((np.transpose(neurons[0]) @ dz[len(dz)-1]))\n",
    "#     print('33333333333333333333333333333333')\n",
    "    nanCheck(d, 'sgd - d')\n",
    "    dW.append(np.multiply(1/m, (np.transpose(neurons[0]) @ d[1])))\n",
    "    nanCheck(dW, 'sgd - dW[1]')\n",
    "    db.append(np.multiply(1/m, np.sum(d[1], axis=0)))\n",
    "    nanCheck(db, 'sgd - db[1]')\n",
    "#     print(dz[0].shape, dz[0])\n",
    "#     print(db[0].shape, db[0])\n",
    "#     print(dz[1].shape, dz[1])\n",
    "#     print(db[1].shape, db[1])\n",
    "#     print('-------dW--------')\n",
    "#     print(dW)\n",
    "#     print('-------db--------')\n",
    "#     print(db)\n",
    "#     print('-------dd--------')\n",
    "    # for i in range(len(weights)):\n",
    "    #     weight = temp[len(temp)-1] @ np.transpose(neurons[-i-1])\n",
    "    #     bias = biases[-i]\n",
    "    #     dW.append(weight)\n",
    "    #     db.append(bias)\n",
    "    #     temp.append(np.transpose(weights[-1]) @ temp[len(temp)-1])\n",
    "    dW.reverse()\n",
    "    db.reverse()\n",
    "    dW += np.multiply(weights, 0.001)\n",
    "    db += np.multiply(biases, 0.001)\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network класс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, classes, layer_neuron_counts=None):\n",
    "        self.layer_neuron_counts = layer_neuron_counts\n",
    "        # Ангилалууд\n",
    "        self.classes = classes\n",
    "        # Weight & Bias\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        # Activation функц ашиглаагүй үеийн утгуудыг хадгалана\n",
    "        self.neurons = []\n",
    "        # Activation функц ашиглаад гарсан утгуудыг хадгалана\n",
    "        self.neurons_activated = []\n",
    "        # Loss утгуудыг хадгалах\n",
    "        self.losses = []\n",
    "\n",
    "        # Weight болон bias-уудыг санамсаргүйгээр үүсгэх\n",
    "        if layer_neuron_counts is not None:\n",
    "            for i in range(1, len(self.layer_neuron_counts)):\n",
    "                prev_layer_size = self.layer_neuron_counts[i - 1]\n",
    "                next_layer_size = self.layer_neuron_counts[i]\n",
    "                self.weights.append(np.random.randn(prev_layer_size, next_layer_size) * 0.1)\n",
    "                self.biases.append(np.random.randn(next_layer_size) * 0.1)\n",
    "\n",
    "    def train(self, training_images, training_labels, epoch, batch_size, learning_rate):\n",
    "        combined = list(zip(training_images, training_labels))\n",
    "        random.shuffle(combined)\n",
    "        training_images[:], training_labels[:] = zip(*combined)\n",
    "        for e in range(1, epoch + 1):\n",
    "            print('Epoch: ' + str(e))\n",
    "            batch_count = round(len(training_images) / batch_size)\n",
    "            losses = []\n",
    "            for b in range(0, batch_count):\n",
    "#                 print('-- Batch: ' + str(b))\n",
    "#                 print('----start----')\n",
    "#                 print(self.weights)\n",
    "#                 print(self.biases)\n",
    "#                 print('-------------')\n",
    "                # print(b * batch_size, (b + 1) * batch_size)\n",
    "                minibatch_data = training_images[b * batch_size:(b + 1) * batch_size][:]\n",
    "#                 print(minibatch_data.shape)\n",
    "                # print(training_images.shape)\n",
    "                # print(minibatch_data.shape)\n",
    "                minibatch_label = training_labels[b * batch_size:(b + 1) * batch_size]\n",
    "                # print(training_labels.shape)\n",
    "                # print(minibatch_label.shape)\n",
    "                # print('---')\n",
    "                loss = self.batch_process(minibatch_data, minibatch_label, learning_rate)\n",
    "#                 print('-----end-----')\n",
    "#                 print(self.weights)\n",
    "#                 print(self.biases)\n",
    "#                 print('-------------')\n",
    "                self.losses.append(loss)\n",
    "                helper.print_progress(b + 1, batch_count)\n",
    "#             self.losses.append(sum(losses))\n",
    "            print('')\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.neurons = [x]\n",
    "        self.neurons_activated = [x]\n",
    "#         print('')\n",
    "#         print('###################')\n",
    "#         [print(str(i) + ', ') for i in self.neurons[0]]\n",
    "#         print('###################')\n",
    "#         print('--------------------------------')\n",
    "#         print(self.weights)\n",
    "#         print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "#         print(self.biases)\n",
    "#         print('================================')\n",
    "        for i, (weights, biases) in enumerate(zip(self.weights, self.biases)):\n",
    "            self.neurons.append(calculate_neuron(relu(self.neurons_activated[len(self.neurons_activated)-1]), weights, biases))\n",
    "            nanCheck(self.neurons, 'forward - self.neurons['+ str(i+1) +']')\n",
    "            if i == len(self.weights) - 1:\n",
    "                self.neurons_activated.append(softmax(self.neurons[len(self.neurons)-1]))\n",
    "            else:\n",
    "                self.neurons_activated.append(relu(self.neurons[len(self.neurons)-1]))\n",
    "            nanCheck(self.neurons_activated, 'forward - self.neurons_activated['+ str(i+1) +']')\n",
    "#         print('NEURONS')\n",
    "#         print(np.array(self.neurons[0]).shape, np.array(self.neurons[1]).shape)\n",
    "#         print(self.neurons)\n",
    "#         print('ACTIVATED')\n",
    "#         print(np.array(self.neurons_activated[0]).shape, np.array(self.neurons_activated[1]).shape)\n",
    "#         print(self.neurons_activated)\n",
    "#         print('neurons')\n",
    "        return self.neurons_activated[len(self.neurons_activated) -1]\n",
    "\n",
    "    def batch_process(self, train_datas, training_labels, learning_rate):\n",
    "        nanCheck(self.weights, 'batch_process - self.weights')\n",
    "        nanCheck(self.biases, 'batch_process - self.biases')\n",
    "        predicted_labels = self.forward(train_datas)\n",
    "        nanCheck(predicted_labels, 'batch_process - predicted_labels')\n",
    "        true_labels = helper.label2vector(self.classes, training_labels)\n",
    "        loss = nll(predicted_labels, true_labels)\n",
    "        nanCheck(loss, 'batch_process - loss')\n",
    "        dW, db = sgd(self.weights, self.biases, self.neurons, self.neurons_activated, predicted_labels, true_labels)\n",
    "        nanCheck(dW, 'batch_process - dW')\n",
    "        nanCheck(db, 'batch_process - db')\n",
    "        # print(np.array(dW).shape)\n",
    "#         self.weights[0] = np.subtract(self.weights[0], dW[0])\n",
    "#         self.biases[0] = np.subtract(self.biases[0], db[0])\n",
    "#         self.weights[1] = np.subtract(self.weights[1], dW[1])\n",
    "#         self.biases[1] = np.subtract(self.biases[1], db[1])\n",
    "#         print(self.weights[0])\n",
    "#         print(self.weights[1])\n",
    "#         print(\"weights1\")\n",
    "#         print(self.weights[0].shape)\n",
    "#         print(self.weights[1].shape)\n",
    "#         print(dW)\n",
    "#         print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "# #         print(dW)\n",
    "# #         [print(dw) for dw in dW[0]]\n",
    "#         print('######################################')\n",
    "#         [print(weights) for weights in self.weights]\n",
    "        temp = self.weights\n",
    "        self.weights = np.subtract(self.weights, [[d*learning_rate for d in dw] for dw in dW])\n",
    "#         print('$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$')\n",
    "#         print(self.weights == temp)\n",
    "#         [print(weights) for weights in self.weights]\n",
    "#         print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
    "        nanCheck(self.weights, 'batch_process - self.weights - dW')\n",
    "#         print(\"weights2\")\n",
    "#         print(self.weights[0].shape)\n",
    "#         print(self.weights[1].shape)\n",
    "#         print('biases1')\n",
    "#         print(self.biases)\n",
    "        self.biases = np.subtract(self.biases, [[d*learning_rate for d in dB] for dB in db])\n",
    "        nanCheck(self.biases, 'batch_process - self.biases + db')\n",
    "#         print('biases2')\n",
    "#         print(self.biases)\n",
    "#         print(db)\n",
    "#         print('-----------')\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "nn = NN(classes, [len(training_images[0]), 32, len(classes)])\n",
    "# nn.train(training_images, training_labels, 10, 128, 0.001)\n",
    "# res = nn.forward(training_images[0])\n",
    "# print(res)\n",
    "# label = np.zeros(10)\n",
    "# label[training_labels[0]] = 1\n",
    "# nll(res, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8705882352941177\n",
      "0.13333333333333333\n",
      "0.16862745098039217\n",
      "-0.012373778003703892\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# for i, d in enumerate(zip(training_images[0], training_images[1], training_images[2])):\n",
    "#     print(i, d)\n",
    "print(training_images[0][181])\n",
    "print(training_images[1][181])\n",
    "print(training_images[2][181])\n",
    "print(nn.weights[0][181][14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.32441012 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.33008657 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.34550331 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n",
      "loss: 0.3752489223327405\n",
      "dw: -0.11207858902017656\n",
      "[[0.         0.32441005 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.3300866  0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.34550334 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n",
      "loss1: 0.37524898809009377\n",
      "dw1: -0.11207859887902491\n",
      "[[0.         0.32441018 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.33008654 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.34550328 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n",
      "loss2: 0.37524885657539353\n",
      "dw2: -0.11207857916132836\n",
      "-0.0989271189960568\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.00001\n",
    "\n",
    "predicted_labels = nn.forward(training_images[0:3][:])\n",
    "true_labels = helper.label2vector(classes, [training_labels[0]])\n",
    "print(np.multiply(true_labels, predicted_labels))\n",
    "loss = nll(predicted_labels, true_labels)\n",
    "print('loss: ' + str(loss))\n",
    "dW, db = sgd(nn.weights, nn.biases, nn.neurons, nn.neurons_activated, predicted_labels, true_labels)\n",
    "dw = dW[0][181][14]\n",
    "print('dw: ' + str(dw))\n",
    "\n",
    "nn.weights[0][181][14] -= (epsilon/2)\n",
    "\n",
    "predicted_labels = nn.forward(training_images[0:3][:])\n",
    "true_labels = helper.label2vector(classes, [training_labels[0]])\n",
    "print(np.multiply(true_labels, predicted_labels))\n",
    "loss1 = nll(predicted_labels, true_labels)\n",
    "print('loss1: ' + str(loss1))\n",
    "dW, db = sgd(nn.weights, nn.biases, nn.neurons, nn.neurons_activated, predicted_labels, true_labels)\n",
    "dw1 = dW[0][181][14]\n",
    "print('dw1: ' + str(dw1))\n",
    "\n",
    "nn.weights[0][181][14] += (epsilon)\n",
    "\n",
    "predicted_labels = nn.forward(training_images[0:3][:])\n",
    "true_labels = helper.label2vector(classes, [training_labels[0]])\n",
    "print(np.multiply(true_labels, predicted_labels))\n",
    "loss2 = nll(predicted_labels, true_labels)\n",
    "print('loss2: ' + str(loss2))\n",
    "dW, db = sgd(nn.weights, nn.biases, nn.neurons, nn.neurons_activated, predicted_labels, true_labels)\n",
    "dw2 = dW[0][181][14]\n",
    "print('dw2: ' + str(dw2))\n",
    "\n",
    "print(dw - ((loss2-loss1)/epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9999999979388119, 2.0611536181901326e-09, 3.442477101374384e-14, 3.1391327855776864e-17]\n",
      "[[ 1.67033067e-05 -2.06108477e-09 -3.44236211e-14 -3.13902792e-17\n",
      "  -3.13902792e-17 -1.67011428e-05 -9.35731036e-14 -5.74933019e-19\n",
      "  -1.02615368e-10 -2.11506038e-19]\n",
      " [-2.06108477e-09  2.06111919e-09 -7.09523712e-23 -6.47001878e-26\n",
      "  -6.47001878e-26 -3.44236211e-14 -1.92868541e-22 -1.18502527e-27\n",
      "  -2.11506038e-19 -4.35946436e-28]\n",
      " [-3.44236211e-14 -7.09523712e-23  3.44241961e-14 -1.08060318e-30\n",
      "  -1.08060318e-30 -5.74933019e-19 -3.22123267e-27 -1.97919376e-32\n",
      "  -3.53251056e-24 -7.28104693e-33]\n",
      " [-3.13902792e-17 -6.47001878e-26 -1.08060318e-30  3.13908036e-17\n",
      "  -9.85382549e-34 -5.24271051e-22 -2.93738398e-30 -1.80479109e-35\n",
      "  -3.22123267e-27 -6.63945539e-36]\n",
      " [-3.13902792e-17 -6.47001878e-26 -1.08060318e-30 -9.85382549e-34\n",
      "   3.13908036e-17 -5.24271051e-22 -2.93738398e-30 -1.80479109e-35\n",
      "  -3.22123267e-27 -6.63945539e-36]\n",
      " [-1.67011428e-05 -3.44236211e-14 -5.74933019e-19 -5.24271051e-22\n",
      "  -5.24271051e-22  1.67011429e-05 -1.56282998e-18 -9.60235926e-24\n",
      "  -1.71385118e-15 -3.53251056e-24]\n",
      " [-9.35731036e-14 -1.92868541e-22 -3.22123267e-27 -2.93738398e-30\n",
      "  -2.93738398e-30 -1.56282998e-18  9.35746666e-14 -5.38000642e-32\n",
      "  -9.60235926e-24 -1.97919376e-32]\n",
      " [-5.74933019e-19 -1.18502527e-27 -1.97919376e-32 -1.80479109e-35\n",
      "  -1.80479109e-35 -9.60235926e-24 -5.38000642e-32  5.74942623e-19\n",
      "  -5.89989344e-29 -1.21605867e-37]\n",
      " [-1.02615368e-10 -2.11506038e-19 -3.53251056e-24 -3.22123267e-27\n",
      "  -3.22123267e-27 -1.71385118e-15 -9.60235926e-24 -5.89989344e-29\n",
      "   1.02617082e-10 -2.17044950e-29]\n",
      " [-2.11506038e-19 -4.35946436e-28 -7.28104693e-33 -6.63945539e-36\n",
      "  -6.63945539e-36 -3.53251056e-24 -1.97919376e-32 -1.21605867e-37\n",
      "  -2.17044950e-29  2.11509571e-19]]\n"
     ]
    }
   ],
   "source": [
    "print(softmax([43,23,12,5]))\n",
    "print(softmax_prime(softmax([43,23,12,5,5,32,13,1,20,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff3d1af02b0>]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
