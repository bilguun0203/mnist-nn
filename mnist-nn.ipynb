{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Neural Network - MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Өгөгдөл бэлдэх"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import mnist\n",
    "import helper\n",
    "% matplotlib inline\n",
    "\n",
    "train_datas, train_labels, test_datas, test_labels = mnist.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    data_norm = data\n",
    "    data_norm = [np.subtract(item, np.mean(item)) for item in data_norm]\n",
    "    data_norm = [np.divide(item, np.std(item)) for item in data_norm]\n",
    "    return np.array(data_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datas_norm = normalize(train_datas)\n",
    "test_datas_norm = normalize(test_datas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/opt/anaconda/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_y = np.array(pd.get_dummies(train_labels).as_matrix())\n",
    "test_y = np.array(pd.get_dummies(test_labels).as_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaN checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_nan(x, msg):\n",
    "    if type(x) is list or type(x) is np.ndarray:\n",
    "        if type(x[0]) is list:\n",
    "            for i, t in enumerate(x):\n",
    "                if np.isnan(np.array(t, dtype=np.float64)).any():\n",
    "                    print(x)\n",
    "                    raise ValueError('NaN ' + str(i) + ': ' + msg)\n",
    "            return None\n",
    "        if type(x[0]) is np.ndarray:\n",
    "            for i, t in enumerate(x):\n",
    "                if np.isnan(t).any():\n",
    "                    print(x)\n",
    "                    raise ValueError('NaN ' + str(i) + ': ' + msg)\n",
    "            return None\n",
    "    try:\n",
    "        if np.isnan(np.array(x, dtype=np.float64)).any():\n",
    "            print(x)\n",
    "            raise ValueError('NaN: ' + msg)\n",
    "    except:\n",
    "        print(x)\n",
    "        raise ValueError('NaN: ' + msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тооцоолол хийхэд хэрэг болох функцууд"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RELU\n",
    "\n",
    "#### Forward\n",
    "\n",
    "\\begin{equation*}\n",
    "relu(y_i) = \\begin{cases} 0, & y_i < 0 \\\\ y_i, & y_i\\geq 0 \\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "#### Backward\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial relu(y_i)}{\\partial y_i} = \\begin{cases} 0, & y_i < 0 \\\\ 1, & y_i\\geq 0 \\end{cases}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ReLU:\n",
    "#     @staticmethod\n",
    "#     def forward(y):\n",
    "#         is_nan(y, 'relu.forward - y')\n",
    "# #         is_nan(y, 'relu.forward - relu(y)')\n",
    "#         return y * (y > 0)\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def backward(y, i=1):\n",
    "#         is_nan(y, 'relu.backward - y')\n",
    "#         return 1 * (y >= 0)\n",
    "# #         return np.heaviside(y, i)\n",
    "\n",
    "def relu(x, derivative=False):\n",
    "    if derivative:\n",
    "        is_nan(x, 'relu.backward - x')\n",
    "        return 1 * (x >= 0)\n",
    "    else:\n",
    "        is_nan(x, 'relu.forward - x')\n",
    "        return x * (x > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "\\begin{equation*}\n",
    "softmax(y_i) = \\frac{e^{y_i}}{\\sum_{j}e^{y_j}}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "#     print(x)\n",
    "    is_nan(x, 'softmax.forward - x')\n",
    "    x = x.T - np.max(x, axis=1)\n",
    "    is_nan(x, 'softmax.forward - x-=max(x)')\n",
    "    y = (np.exp(x) / np.sum(np.exp(x),axis=0)).T\n",
    "    is_nan(y, 'softmax.forward - y')\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Log Likelihood\n",
    "\n",
    "- Y* - жинхэнэ ангилал\n",
    "- Y - бодсон ангилал\n",
    "- n - сургалтын өгөгдлийн тоо\n",
    "\n",
    "#### Forward\n",
    "\n",
    "\\begin{equation*}\n",
    "L = - \\sum_{n} \\sum_{i} Y_{ni}^* log(Y_{ni})\n",
    "\\end{equation*}\n",
    "\n",
    "#### Backward\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial L}{\\partial z_i} = Y_i - Y_{i}^*\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(P, Y, derivative=False):\n",
    "    is_nan(P, 'nll - P')\n",
    "    is_nan(Y, 'nll - Y')\n",
    "    P = np.array(P)\n",
    "    Y = np.array(Y)\n",
    "    if derivative:\n",
    "        return P - Y\n",
    "    else:\n",
    "#         print('[0 -- 10]')\n",
    "#         print(P[0:10])\n",
    "#         print('[100 -- 110]')\n",
    "#         print(P[100:110])\n",
    "        l = np.log(P) * Y\n",
    "        is_nan(l, 'nll.forward - l')\n",
    "        return -np.mean(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, classes, layer_neuron_counts):\n",
    "        # Давхарга бүрийн нейроны тоо (жишээ нь [784, 64, 10])\n",
    "        self.layer_neuron_counts = layer_neuron_counts\n",
    "        # Ангилалууд\n",
    "        self.classes = classes\n",
    "        # Weight & Bias\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        # Activation функц ашиглаагүй үеийн утгуудыг хадгалана\n",
    "        self.layers = []\n",
    "        # Activation функц ашиглаад гарсан утгуудыг хадгалана\n",
    "        self.layers_activated = []\n",
    "        # Loss утгуудыг хадгалах\n",
    "        self.loss = []\n",
    "\n",
    "        # Weight болон bias-уудыг санамсаргүйгээр үүсгэх\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        for i in range(1, len(self.layer_neuron_counts)):\n",
    "            prev_layer_size = self.layer_neuron_counts[i - 1]\n",
    "            next_layer_size = self.layer_neuron_counts[i]\n",
    "            self.weights.append(np.random.randn(prev_layer_size, next_layer_size) * 0.1)\n",
    "            self.biases.append(np.random.randn(next_layer_size) * 0.1)\n",
    "\n",
    "    def calculate_layer(self, i, activation='relu'):\n",
    "        is_nan(self.layers_activated[i], 'calculate_layer - x')\n",
    "        # Activation функц ашиглахаас өмнөх утга (H = WX + b)\n",
    "        z = self.layers_activated[i] @ self.weights[i] + self.biases[i]\n",
    "        # Activation функц ашигласны дараах утга (relu эсвэл softmax)\n",
    "        a = None\n",
    "        if activation == 'relu':\n",
    "            a = relu(z)\n",
    "        elif activation == 'softmax':\n",
    "            a = softmax(z)\n",
    "        return z, a\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.layers = [x]\n",
    "        self.layers_activated = [x]\n",
    "        for i in range(len(self.layer_neuron_counts) - 1):\n",
    "            # Activation функц ашиглахаас өмнөх утга (H = WX + b)\n",
    "            z = None\n",
    "            # Activation функц ашигласны дараах утга (relu(H) эсвэл softmax(H))\n",
    "            a = None\n",
    "            if i == len(self.layer_neuron_counts) - 2:\n",
    "                z, a = self.calculate_layer(i, 'softmax')\n",
    "            else:\n",
    "                z, a = self.calculate_layer(i, 'relu')\n",
    "            self.layers.append(z)\n",
    "            self.layers_activated.append(a)\n",
    "            is_nan(self.layers[len(self.layers) - 1],\n",
    "                   'forward-' + str(i) + ' - self.layers[' + str(len(self.layers) - 1) + ']')\n",
    "            is_nan(self.layers_activated[len(self.layers_activated) - 1],\n",
    "                   'forward-' + str(i) + ' - self.layers_activated[' + str(len(self.layers) - 1) + ']')\n",
    "        return self.layers_activated[-1]\n",
    "\n",
    "    def backward(self, prediction, target):\n",
    "        dW = []\n",
    "        dB = []\n",
    "        # ---\n",
    "        d = [nll(prediction, target, derivative=True)]\n",
    "        m = len(d)\n",
    "        is_nan(d, 'sgd - d[0]')\n",
    "        dW.append(np.transpose(self.layers_activated[1]) @ d[0])\n",
    "        is_nan(dW, 'sgd - dW[0]')\n",
    "        dB.append(np.sum(d[0], axis=0))\n",
    "        is_nan(dB, 'sgd - db[0]')\n",
    "        # ---\n",
    "        d.append(np.multiply(d[0] @ np.transpose(self.weights[1]), relu(self.layers_activated[1], True)))\n",
    "        is_nan(d, 'sgd - d[1]')\n",
    "        dW.append(np.transpose(self.layers_activated[0]) @ d[1])\n",
    "        is_nan(dW, 'sgd - dW[1]')\n",
    "        dB.append(np.sum(d[1], axis=0))\n",
    "        is_nan(dB, 'sgd - db[1]')\n",
    "        dW.reverse()\n",
    "        dB.reverse()\n",
    "        return np.multiply(1/m, dW), np.multiply(1/m, dB)\n",
    "\n",
    "    def train(self, train_x, train_y, epoch, batch_size, learning_rate):\n",
    "        index = np.arange(train_x.shape[0])\n",
    "        # Epoch\n",
    "        for e in range(1, epoch + 1):\n",
    "            print('Epoch: ' + str(e) + '/' + str(epoch))\n",
    "            batch_count = round(len(train_y) / batch_size)\n",
    "            np.random.shuffle(index)\n",
    "            epoch_loss = []\n",
    "            # Batch\n",
    "            for b in range(0, batch_count):\n",
    "                # Batch болгон хуваах\n",
    "                l = b * batch_size\n",
    "                r = l + batch_size\n",
    "                batch_x = [train_x[i] for i in index[l:r]]\n",
    "                batch_y = [train_y[i] for i in index[l:r]]\n",
    "                # Batch-г боловсруулах\n",
    "                dW, dB, loss = self.batch_process(batch_x, batch_y)\n",
    "                # Batch болгоны loss-г хадгалах\n",
    "                epoch_loss.append(loss)\n",
    "                self.loss.append(loss)\n",
    "                # Параметруудыг шинэчлэх\n",
    "                self.update_parameters(dW, dB, learning_rate)\n",
    "\n",
    "                helper.print_progress(b + 1, batch_count, loss=np.mean(epoch_loss))\n",
    "            print('')\n",
    "\n",
    "    def batch_process(self, train_x, train_y):\n",
    "        is_nan(self.weights, 'batch_process - self.weights')\n",
    "        is_nan(self.biases, 'batch_process - self.biases')\n",
    "        # Forward propagation\n",
    "        predicted_y = self.forward(train_x)\n",
    "        is_nan(predicted_y, 'batch_process - predicted_y')\n",
    "        # Loss тооцох\n",
    "        loss = nll(predicted_y, train_y)\n",
    "        is_nan(loss, 'batch_process - loss')\n",
    "        # Backward propagation\n",
    "        dW, dB = self.backward(predicted_y, train_y)\n",
    "        return dW, dB, loss\n",
    "\n",
    "    def update_parameters(self, dW, dB, learning_rate):\n",
    "#         [print(i) for i in dW]\n",
    "        # Update weights\n",
    "        self.weights = np.subtract(self.weights, np.multiply(learning_rate, dW))\n",
    "        is_nan(self.weights, 'batch_process - self.weights - dW')\n",
    "        # Update biases\n",
    "        self.biases = np.subtract(self.biases, np.multiply(learning_rate, dB))\n",
    "        is_nan(self.biases, 'batch_process - self.biases - db')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5\n",
      "[============================================================] 100.0% 400/400 loss: 0.04670504348248124\n",
      "Epoch: 2/5\n",
      "[============================================================] 100.0% 400/400 loss: 0.02605843771651462\n",
      "Epoch: 3/5\n",
      "[============================================================] 100.0% 400/400 loss: 0.024504706381365857\n",
      "Epoch: 4/5\n",
      "[============================================================] 100.0% 400/400 loss: 0.02449502141601416\n",
      "Epoch: 5/5\n",
      "[============================================================] 100.0% 400/400 loss: 0.024897315345994108\n"
     ]
    }
   ],
   "source": [
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "nn = NN(classes, [len(train_datas_norm[0]), 300, len(classes)])\n",
    "nn.train(train_datas_norm, train_y, 5, 150, 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8980ef3cf8>]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(nn.layers_activated[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = nn.forward([test_images_norm[4]])\n",
    "ans = helper.one_hot_encoding(nn.classes, [test_labels[4]])\n",
    "print(res, np.sum(res))\n",
    "print(NLL.forward(res, ans))\n",
    "print(ans)\n",
    "print(test_labels[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, d in enumerate(zip(training_images[0], training_images[1], training_images[2])):\n",
    "#     print(i, d)\n",
    "print(training_images[0][181])\n",
    "print(training_images[1][181])\n",
    "print(training_images[2][181])\n",
    "print(nn.weights[0][181][14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.00001\n",
    "\n",
    "predicted_labels = nn.forward(training_images[0:3][:])\n",
    "true_labels = helper.label2vector(classes, [training_labels[0]])\n",
    "print(np.multiply(true_labels, predicted_labels))\n",
    "loss = NLL.forward(predicted_labels, true_labels)\n",
    "print('loss: ' + str(loss))\n",
    "dW, db = sgd(nn.weights, nn.biases, nn.layers, nn.layers_activated, predicted_labels, true_labels)\n",
    "dw = dW[0][181][14]\n",
    "print('dw: ' + str(dw))\n",
    "\n",
    "nn.weights[0][181][14] -= (epsilon/2)\n",
    "\n",
    "predicted_labels = nn.forward(training_images[0:3][:])\n",
    "true_labels = helper.label2vector(classes, [training_labels[0]])\n",
    "print(np.multiply(true_labels, predicted_labels))\n",
    "loss1 = NLL.forward(predicted_labels, true_labels)\n",
    "print('loss1: ' + str(loss1))\n",
    "dW, db = nn.backward(predicted_labels, true_labels)\n",
    "dw1 = dW[0][181][14]\n",
    "print('dw1: ' + str(dw1))\n",
    "\n",
    "nn.weights[0][181][14] += epsilon\n",
    "\n",
    "predicted_labels = nn.forward(training_images[0:3][:])\n",
    "true_labels = helper.label2vector(classes, [training_labels[0]])\n",
    "print(np.multiply(true_labels, predicted_labels))\n",
    "loss2 = NLL.forward(predicted_labels, true_labels)\n",
    "print('loss2: ' + str(loss2))\n",
    "dW, db = nn.backward(predicted_labels, true_labels)\n",
    "dw2 = dW[0][181][14]\n",
    "print('dw2: ' + str(dw2))\n",
    "\n",
    "print(dw - ((loss2-loss1)/epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = [1000, 0.3, 2, 0.5, 0.9, 1.1, 1.5, 0.5, 0.01, 0.09]\n",
    "\n",
    "# print(softmax.forward(ll))\n",
    "print(LogSoftmax.forward(ll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(training_images))\n",
    "print(np.max(training_images))\n",
    "print(np.min(test_images))\n",
    "print(np.max(test_images))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
